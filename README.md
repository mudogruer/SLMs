# Small Language Models in Education (SLMs)

This repository contains the code and documentation for the project "Small Language Models in Education", focused on fine-tuning Small Language Models (SLMs) for generating and evaluating scientific multiple-choice questions (MCQs). Our work explores the use of Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) techniques to enhance the capabilities of SLMs in educational settings.

## Overview

The project aims to demonstrate the potential of SLMs, particularly the Mixtral model, when fine-tuned with educational datasets like SciQ and MedMCQA. We evaluate the performance of these models in generating accurate and format-aligned MCQs, providing insights into their application in automated assessment and personalized learning.

## Prerequisites

Ensure you have the following installed:
    Python 3.8 or later
    PyTorch
    Transformers

## Datasets

    SciQ: A dataset for science exam MCQs. Download (https://allenai.org/data/sciq).
    MedMCQA: A medical domain MCQ dataset. Download (https://github.com/medmcqa/medmcqa).

## Getting Started

To get started with this project, clone the repository to your local machine:

```bash
git clone https://github.com/mudogruer/SLMs.git



